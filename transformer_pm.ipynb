{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "transformer_pm (1).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "A28M2i0vnU-k",
        "cY0hso6pnU-o",
        "OEF9r7ljnU-s",
        "TIkIMZJ3nU-x",
        "wvqb3wHunU-0",
        "EXaAPEtTnU-4",
        "_l-iKTbxnU--",
        "xSiIYu3nnVAW"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhUfPHV8nU9w",
        "colab_type": "text"
      },
      "source": [
        "# Individual Project\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZr4-s0bnU9x",
        "colab_type": "text"
      },
      "source": [
        "## Preparing the Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6yNQArRnU9y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchtext\n",
        "from torchtext.datasets import TranslationDataset, Multi30k\n",
        "from torchtext.data import Field, BucketIterator\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHv4pDGqnU94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jcb30GT8nU98",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "outputId": "54c8be50-700f-4270-9d88-61d7da92aec6"
      },
      "source": [
        "!spacy download en_core_web_sm\n",
        "!spacy link en_core_web_sm en\n",
        "\n",
        "spacy_en = spacy.load('en')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.7.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (49.6.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\n",
            "\u001b[38;5;1m✘ Link 'en' already exists\u001b[0m\n",
            "To overwrite an existing link, use the --force flag\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEBm55fynU9_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "def tokenize_en(text):\n",
        "    \"\"\"\n",
        "    Tokenizes English text from a string into a list of strings\n",
        "    \"\"\"\n",
        "    res = []\n",
        "    for tok in spacy_en.tokenizer(text):\n",
        "        res.append(tok.text)\n",
        "        if len(res)>512:\n",
        "            break\n",
        "    return res\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LbOJ-0PnU-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT = Field(tokenize = tokenize_en, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True, \n",
        "            batch_first = True)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lm3sBKTVeTb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "02b9c29c-3837-4185-e11c-e35444482de5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vE1elqFfnU-Q",
        "colab_type": "text"
      },
      "source": [
        "We then load the dataset and build the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ondA-SanU-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext.data import Field, Example, TabularDataset, BucketIterator\n",
        "import sys\n",
        "import csv\n",
        "\n",
        "fields = [('LABEL', TEXT), ('INPUT_TEXT', TEXT)]\n",
        "train_data, valid_data,test_data = TabularDataset.splits(\n",
        "    path='/content/drive/My Drive/pubmed/', format='csv',\n",
        "    train='pm_train.csv',validation='pm_val.csv', test='pm_test.csv',\n",
        "    skip_header=True, fields=fields)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4uV72h9STUE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uYY0OaanU-W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT.build_vocab(train_data, min_freq = 2)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RJrnFHDnU-c",
        "colab_type": "text"
      },
      "source": [
        "Define the device and the data iterator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sg583fjfnU-c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-KZ_g0OnU-g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 4\n",
        "\n",
        "train_iterator,valid_iterator,test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data,test_data), \n",
        "     batch_size = BATCH_SIZE,\n",
        "     sort=False,\n",
        "     device = device)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1J-9TaSRGHVu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A28M2i0vnU-k",
        "colab_type": "text"
      },
      "source": [
        "## Building the Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kbsocdPnU-l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 n_heads, \n",
        "                 pf_dim,\n",
        "                 dropout, \n",
        "                 device,\n",
        "                 max_length = 10000):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = device\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([EncoderLayer(hid_dim, \n",
        "                                                  n_heads, \n",
        "                                                  pf_dim,\n",
        "                                                  dropout, \n",
        "                                                  device) \n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        #src_mask = [batch size, src len]\n",
        "        \n",
        "        batch_size = src.shape[0]\n",
        "        src_len = src.shape[1]\n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "        \n",
        "        #pos = [batch size, src len]\n",
        "        \n",
        "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "            \n",
        "        #src = [batch size, src len, hid dim]\n",
        "            \n",
        "        return src"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHNl9UtSnU-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, \n",
        "                 hid_dim, \n",
        "                 n_heads, \n",
        "                 pf_dim,  \n",
        "                 dropout, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
        "                                                                     pf_dim, \n",
        "                                                                     dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        #src_mask = [batch size, src len]\n",
        "                \n",
        "        #self attention\n",
        "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        #positionwise feedforward\n",
        "        _src = self.positionwise_feedforward(src)\n",
        "        \n",
        "        #dropout, residual and layer norm\n",
        "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        return src"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7lGOggmnU-s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        assert hid_dim % n_heads == 0\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = hid_dim // n_heads\n",
        "        \n",
        "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "        \n",
        "    def forward(self, query, key, value, mask = None):\n",
        "        \n",
        "        batch_size = query.shape[0]\n",
        "        \n",
        "        #query = [batch size, query len, hid dim]\n",
        "        #key = [batch size, key len, hid dim]\n",
        "        #value = [batch size, value len, hid dim]\n",
        "                \n",
        "        Q = self.fc_q(query)\n",
        "        K = self.fc_k(key)\n",
        "        V = self.fc_v(value)\n",
        "        \n",
        "        #Q = [batch size, query len, hid dim]\n",
        "        #K = [batch size, key len, hid dim]\n",
        "        #V = [batch size, value len, hid dim]\n",
        "                \n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        \n",
        "        #Q = [batch size, n heads, query len, head dim]\n",
        "        #K = [batch size, n heads, key len, head dim]\n",
        "        #V = [batch size, n heads, value len, head dim]\n",
        "                \n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "        \n",
        "        #energy = [batch size, n heads, query len, key len]\n",
        "        \n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, -1e10)\n",
        "        \n",
        "        attention = torch.softmax(energy, dim = -1)\n",
        "                \n",
        "        #attention = [batch size, n heads, query len, key len]\n",
        "                \n",
        "        x = torch.matmul(self.dropout(attention), V)\n",
        "        \n",
        "        #x = [batch size, n heads, query len, head dim]\n",
        "        \n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        \n",
        "        #x = [batch size, query len, n heads, head dim]\n",
        "        \n",
        "        x = x.view(batch_size, -1, self.hid_dim)\n",
        "        \n",
        "        #x = [batch size, query len, hid dim]\n",
        "        \n",
        "        x = self.fc_o(x)\n",
        "        \n",
        "        #x = [batch size, query len, hid dim]\n",
        "        \n",
        "        return x, attention"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJ3bl7n8nU-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionwiseFeedforwardLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, pf_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
        "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        #x = [batch size, seq len, hid dim]\n",
        "        \n",
        "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
        "        \n",
        "        #x = [batch size, seq len, pf dim]\n",
        "        \n",
        "        x = self.fc_2(x)\n",
        "        \n",
        "        #x = [batch size, seq len, hid dim]\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ry6-0oZOnU-1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 output_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 n_heads, \n",
        "                 pf_dim, \n",
        "                 dropout, \n",
        "                 device,\n",
        "                 max_length = 10000\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.device = device\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([DecoderLayer(hid_dim, \n",
        "                                                  n_heads, \n",
        "                                                  pf_dim, \n",
        "                                                  dropout, \n",
        "                                                  device)\n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        \n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        \n",
        "        #trg = [batch size, trg len]\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "        #trg_mask = [batch size, trg len]\n",
        "        #src_mask = [batch size, src len]\n",
        "                \n",
        "        batch_size = trg.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        \n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "                            \n",
        "        #pos = [batch size, trg len]\n",
        "        #print('tok_embedding',self.tok_embedding(trg))\n",
        "        #print('pos_embedding',self.pos_embedding(pos))   \n",
        "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
        "                \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        output = self.fc_out(trg)\n",
        "        \n",
        "        #output = [batch size, trg len, output dim]\n",
        "            \n",
        "        return output, attention"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-G--mXjnU-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, \n",
        "                 hid_dim, \n",
        "                 n_heads, \n",
        "                 pf_dim, \n",
        "                 dropout, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
        "                                                                     pf_dim, \n",
        "                                                                     dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "        #trg_mask = [batch size, trg len]\n",
        "        #src_mask = [batch size, src len]\n",
        "        \n",
        "        #self attention\n",
        "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
        "            \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "            \n",
        "        #encoder attention\n",
        "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
        "                    \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        \n",
        "        #positionwise feedforward\n",
        "        _trg = self.positionwise_feedforward(trg)\n",
        "        \n",
        "        #dropout, residual and layer norm\n",
        "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        return trg, attention"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vx-0r1TYnU--",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, \n",
        "                 encoder, \n",
        "                 decoder, \n",
        "                 src_pad_idx, \n",
        "                 trg_pad_idx, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device\n",
        "        \n",
        "    def make_src_mask(self, src):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        \n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "\n",
        "        return src_mask\n",
        "    \n",
        "    def make_trg_mask(self, trg):\n",
        "        \n",
        "        #trg = [batch size, trg len]\n",
        "        \n",
        "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        \n",
        "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
        "        \n",
        "        trg_len = trg.shape[1]\n",
        "        \n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
        "        \n",
        "        #trg_sub_mask = [trg len, trg len]\n",
        "            \n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\n",
        "        \n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        \n",
        "        return trg_mask\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        #trg = [batch size, trg len]\n",
        "                \n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        \n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        \n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        #print('enc_src size:',enc_src.shape)\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "                \n",
        "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        #output = [batch size, trg len, output dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        return output, attention"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnmOuDIanU_C",
        "colab_type": "text"
      },
      "source": [
        "## Training the Seq2Seq Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ro-DaZYnU_C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "78f55468-8e9a-4868-971c-4660e382ebd7"
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "OUTPUT_DIM = len(TEXT.vocab)\n",
        "HID_DIM = 256\n",
        "ENC_LAYERS = 4\n",
        "DEC_LAYERS = 4\n",
        "ENC_HEADS = 8\n",
        "DEC_HEADS = 8\n",
        "ENC_PF_DIM = 512\n",
        "DEC_PF_DIM = 512\n",
        "ENC_DROPOUT = 0.1\n",
        "DEC_DROPOUT = 0.1\n",
        "\n",
        "enc = Encoder(INPUT_DIM, \n",
        "              HID_DIM, \n",
        "              ENC_LAYERS, \n",
        "              ENC_HEADS, \n",
        "              ENC_PF_DIM, \n",
        "              ENC_DROPOUT, \n",
        "              device)\n",
        "\n",
        "dec = Decoder(OUTPUT_DIM, \n",
        "              HID_DIM, \n",
        "              DEC_LAYERS, \n",
        "              DEC_HEADS, \n",
        "              DEC_PF_DIM, \n",
        "              DEC_DROPOUT, \n",
        "              device)\n",
        "\n",
        "print(INPUT_DIM)\n",
        "print(OUTPUT_DIM)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "169538\n",
            "169538\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itQUdhCLnU_F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SRC_PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "TRG_PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tP7wI5dnU_J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "da24da32-82cc-4dbb-b8ef-f01ab75a49b5"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 140,766,274 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zu2LUFMGnU_N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxbCEsw1nU_Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.apply(initialize_weights);"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwst5jVUnU_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LEARNING_RATE = 0.0005\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nhf3m9zJnU_X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOnyrwkTnU_a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src = batch.INPUT_TEXT\n",
        "        trg = batch.LABEL\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        #print(src.shape)\n",
        "        #print(trg.shape)\n",
        "        output, _ = model(src, trg[:,:-1])\n",
        "        #print(output.shape)\n",
        "                \n",
        "        #output = [batch size, trg len - 1, output dim]\n",
        "        #trg = [batch size, trg len]\n",
        "            \n",
        "        output_dim = output.shape[-1]\n",
        "            \n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        trg = trg[:,1:].contiguous().view(-1)\n",
        "                \n",
        "        #output = [batch size * trg len - 1, output dim]\n",
        "        #trg = [batch size * trg len - 1]\n",
        "            \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-B5CjcTnU_d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.INPUT_TEXT\n",
        "            trg = batch.LABEL\n",
        "\n",
        "            output, _ = model(src, trg[:,:-1])\n",
        "            \n",
        "            #output = [batch size, trg len - 1, output dim]\n",
        "            #trg = [batch size, trg len]\n",
        "            \n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            trg = trg[:,1:].contiguous().view(-1)\n",
        "            \n",
        "            #output = [batch size * trg len - 1, output dim]\n",
        "            #trg = [batch size * trg len - 1]\n",
        "            \n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9WUJoXinU_g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueU67NIqnU_k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "b58959e1-614d-45e2-cf1d-ac7ce5c9151b"
      },
      "source": [
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "LEARNING_RATE = 0.00001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
        "best_valid_loss = float('inf')\n",
        "train_loss_list = []\n",
        "valid_loss_list = []\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    train_loss_list.append(train_loss)\n",
        "    valid_loss_list.append(valid_loss)\n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), '/content/drive/My Drive/pubmed/transformer_pm_1.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} ')\n",
        "    print(f'\\tValid Loss: {valid_loss:.3f} ')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 33m 37s\n",
            "\tTrain Loss: 6.660 \n",
            "\tValid Loss: 5.858 \n",
            "Epoch: 02 | Time: 33m 42s\n",
            "\tTrain Loss: 5.699 \n",
            "\tValid Loss: 5.452 \n",
            "Epoch: 03 | Time: 33m 40s\n",
            "\tTrain Loss: 5.336 \n",
            "\tValid Loss: 5.256 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AL98_Rny_oll",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "1124230b-b985-49be-e0e2-e6b06a62d573"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure()\n",
        "plt.plot(range(len(train_loss_list)),train_loss_list,label = 'train loss')\n",
        "plt.plot(range(len(valid_loss_list)),valid_loss_list,label = 'validation loss')\n",
        "plt.xlabel('epchos')\n",
        "plt.ylabel('Total loss')\n",
        "plt.legend()\n",
        "plt.savefig('/content/drive/My Drive/pubmed/tf_tran_loss.png')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Pq-hGgIBYmt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-m3CKCS_ot_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7mf9_SnnU_n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c38d6108-66b8-4208-bf75-44564bbaae2d"
      },
      "source": [
        "\n",
        "\n",
        "model.load_state_dict(torch.load('/content/drive/My Drive/pubmed/transformer_pm.pt'))\n",
        "\n",
        "# test_loss = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "# print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwNGysNqwA05",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "a6ebedd5-6a8b-41fa-bdc0-a39fcde27a65"
      },
      "source": [
        "pip install rouge"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rouge\n",
            "  Downloading https://files.pythonhosted.org/packages/43/cc/e18e33be20971ff73a056ebdb023476b5a545e744e3fc22acd8c758f1e0d/rouge-1.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rouge) (1.15.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7TiwXnpv52J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "a4260895-a432-47ab-dba8-a09eabbdc54b"
      },
      "source": [
        "from rouge import Rouge\n",
        "import numpy as np\n",
        "\n",
        "rouge1_scores = []\n",
        "rouge2_scores = []\n",
        "rougel_scores = []\n",
        "rouge = Rouge()\n",
        "for example_idx in range(len(valid_data.examples)):\n",
        "    src = vars(valid_data.examples[example_idx])['INPUT_TEXT']\n",
        "    trg = vars(valid_data.examples[example_idx])['LABEL']\n",
        "    translation, attention = translate_sentence(src, TEXT, TEXT, model, device)\n",
        "    reference = ' '.join(trg)\n",
        "    prediction = ' '.join(translation)\n",
        "    try:\n",
        "        scores = rouge.get_scores(prediction, reference)\n",
        "        rouge1_scores.append(scores[0]['rouge-1']['f'])\n",
        "        rouge2_scores.append(scores[0]['rouge-2']['f'])\n",
        "        rougel_scores.append(scores[0]['rouge-l']['f'])\n",
        "    except:\n",
        "        continue\n",
        "print('ROUGE-1 score:{},std: {}'.format(np.mean(np.array(rouge1_scores)),np.std(np.array(rouge1_scores))))\n",
        "print('ROUGE-2 score:{},std: {}'.format(np.mean(np.array(rouge2_scores)),np.std(np.array(rouge2_scores))))\n",
        "print('ROUGE-l score:{},std: {}'.format(np.mean(np.array(rougel_scores)),np.std(np.array(rougel_scores))))\n",
        "print(len(rouge1_scores))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROUGE-1 score:0.21834715358912055,std: 0.07808335391667952\n",
            "ROUGE-2 score:0.040924661736485325,std: 0.027976401733488573\n",
            "ROUGE-l score:0.2241385393040221,std: 0.05917401041021015\n",
            "2500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6STyLzpHnU_r",
        "colab_type": "text"
      },
      "source": [
        "## Inference\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhpIXdzonU_r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 512):\n",
        "    \n",
        "    model.eval()\n",
        "        \n",
        "    if isinstance(sentence, str):\n",
        "        nlp = spacy.load('de')\n",
        "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
        "    else:\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "\n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
        "        \n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
        "\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
        "    \n",
        "    src_mask = model.make_src_mask(src_tensor)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        enc_src = model.encoder(src_tensor, src_mask)\n",
        "\n",
        "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
        "\n",
        "    for i in range(max_len):\n",
        "\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
        "\n",
        "        trg_mask = model.make_trg_mask(trg_tensor)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        pred_token = output.argmax(2)[:,-1].item()\n",
        "        \n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "            break\n",
        "    \n",
        "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
        "    \n",
        "    return trg_tokens[1:], attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WTwmSn2nU_u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def display_attention(sentence, translation, attention, n_heads = 8, n_rows = 4, n_cols = 2):\n",
        "    \n",
        "    assert n_rows * n_cols == n_heads\n",
        "    \n",
        "    fig = plt.figure(figsize=(15,25))\n",
        "    \n",
        "    for i in range(n_heads):\n",
        "        \n",
        "        ax = fig.add_subplot(n_rows, n_cols, i+1)\n",
        "        \n",
        "        _attention = attention.squeeze(0)[i].cpu().detach().numpy()\n",
        "\n",
        "        cax = ax.matshow(_attention, cmap='bone')\n",
        "\n",
        "        ax.tick_params(labelsize=12)\n",
        "        ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'], \n",
        "                           rotation=45)\n",
        "        ax.set_yticklabels(['']+translation)\n",
        "\n",
        "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxxfRk5WPwB0",
        "colab_type": "text"
      },
      "source": [
        "An example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-CQJwzwnVAM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "91a548e7-fba1-497d-8078-8e79467f2ceb"
      },
      "source": [
        "example_idx = 50\n",
        "\n",
        "src = vars(test_data.examples[example_idx])['INPUT_TEXT']\n",
        "trg = vars(test_data.examples[example_idx])['LABEL']\n",
        "print(len(test_data.examples))\n",
        "print(f'src = {src}')\n",
        "print(f'trg = {trg}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2500\n",
            "src = ['the', 'nlrp3', 'inflammasome', ',', 'a', 'member', 'of', 'the', 'nlr', 'family', ',', 'is', 'a', 'key', 'player', 'in', 'the', 'production', 'of', 'uric', 'acid', '-', 'mediated', 'il-1', 'and', 'is', 'an', 'important', 'cytoplasmic', 'protein', 'complex', 'involved', 'in', 'gouty', 'inflammation', '(', '12', ')', '.', 'although', 'the', 'precise', 'pathogenic', 'mechanism', 'of', 'gout', 'has', 'not', 'been', 'clearly', 'determined', ',', 'several', 'crucial', 'proteins', 'such', 'as', 'the', 'purinergic', 'receptor', 'p2x', 'ligand', '-', 'gated', 'ion', 'channel', '7', '(', 'p2x7r', ')', '(', '345', ')', 'and', 'caspase', 'activation', 'and', 'recruitment', 'domain', '8', '(', 'card8', ')', '(', '67', ')', 'proteins', 'are', 'known', 'to', 'be', 'responsible', 'for', 'the', 'pathogenesis', 'of', 'gout', '.', 'recent', 'single', '-', 'nucleotide', 'polymorphism', '(', 'snp', ')', 'studies', 'suggested', 'that', 'genetic', 'alternations', 'of', 'several', 'target', 'molecules', 'such', 'as', 'card8', 'and', 'p2x7r', 'contribute', 'to', 'the', 'process', 'of', 'nlrp3', 'inflammasome', 'activation', '.', 'genetic', 'variants', 'of', 'card8', 'were', 'identified', 'to', 'play', 'a', 'role', 'in', 'the', 'pathogenesis', 'of', 'a', 'variety', 'of', 'inflammatory', 'diseases', ',', 'such', 'as', 'rheumatoid', 'arthritis', '(', 'ra', ')', '(', '8)', 'and', 'inflammatory', 'bowel', 'disease', '(', 'ibd', ')', '(', '9', ')', '.', 'chen', 'et', 'al', '.', '(', '6', ')', 'demonstrated', 'a', 'significantly', 'different', 'genotypic', 'distribution', 'of', 'the', 'card8', 'rs2043211', 'polymorphism', 'between', 'gout', 'and', 'control', 'patients', 'in', 'the', 'chinese', 'population', '.', 'another', 'candidate', 'gene', 'involved', 'in', 'gouty', 'inflammation', 'might', 'be', 'p2x7r', ',', 'whose', 'protein', 'product', 'binds', 'with', 'atp', 'and', 'induces', 'the', 'efflux', 'of', 'k', 'ions', 'through', 'the', 'p2x7r', 'channel', 'from', 'cells', ',', 'finally', 'triggering', 'activation', 'of', 'the', 'nlrp3', 'inflammasome', '(', '34', ')', '.', 'a', 'loss', '-', 'of', '-', 'function', '(', '1513', 'a', '>', 'c', ')', 'snp', 'of', 'the', 'p2x7r', 'gene', 'affects', 'atp', '-', 'induced', 'cellular', 'functions', 'such', 'as', 'apoptotic', 'cell', 'death', '(', '10', ')', 'and', 'il-1', 'release', '(', '11', ')', '.', 'more', 'evidence', 'to', 'support', 'the', 'intimate', 'relationship', 'of', 'p2x7r', 'gene', 'polymorphisms', 'with', 'autoimmune', 'diseases', 'are', 'the', 'demonstrated', 'links', 'between', 'p2x7r', 'polymorphisms', 'and', 'ra', 'and', 'systemic', 'lupus', 'erythematosus', '(', 'sle', ')', '(', '1213', ')', '.', 'however', ',', 'there', 'are', 'no', 'available', 'data', 'linking', 'p2x7r', 'snps', 'with', 'gout', '.', 'here', ',', 'we', 'investigated', 'the', 'association', 'of', 'p2x7r', 'rs3751142', 'and', 'card8', 'rs2043211', 'polymorphisms', 'with', 'the', 'susceptibility', 'and', 'clinical', 'manifestations', 'of', 'gout', 'in', 'the', 'male', 'korean', 'population', '.', 'a', 'total', 'of', '242', 'male', 'gout', 'patients', 'fulfilled', 'the', 'preliminary', 'criteria', 'for', 'classification', 'of', 'primary', 'gout', 'proposed', 'by', 'the', 'american', 'college', 'of', 'rheumatology', '(', '14', ')', 'and', 'a', 'total', 'of', '280', 'healthy', 'male', 'controls', 'were', 'consecutively', 'enrolled', 'in', 'this', 'study', '.', 'clinical', 'and', 'laboratory', 'variables', ',', 'including', 'age', 'at', 'the', 'time', 'of', 'study', ',', 'disease', 'onset', 'age', ',', 'body', 'mass', 'index', '(', 'bmi', ')', ',', 'disease', 'duration', ',', 'and', 'serum', 'uric', 'acid', 'level', 'were', 'identified', 'by', 'individual', 'interviews', 'with', 'each', 'patient', 'and', 'medical', 'record', 'review', '.', 'medications', 'including', 'corticosteroids', ',', 'non', '-', 'steroidal', 'anti', '-', 'inflammatory', 'drugs', '(', 'nsaids', ')', ',', 'colchicine', ',', 'allopurinol', ',', 'and', 'benzbromarone', 'that', 'were', 'used', 'for', 'gout', 'treatment', 'within', 'one', 'month', 'of', 'the', 'study', 'onset', 'were', 'evaluated', 'through', 'medical', 'record', 'reviews', '.', 'the', 'study', 'protocol', 'was', 'reviewed', 'and', 'approved', 'by', 'the', 'institutional', 'review', 'board', '/', 'ethics', 'committee', 'at', 'each', 'medical', 'center', 'that', 'participated', 'in', 'this', 'study', '.', 'assay', 'reagents', 'for', 'rs3751142(c', '>', 'a', ')', 'in', 'the', 'p2x7r']\n",
            "trg = [' ', 'the', 'aim', 'of', 'this', 'study', 'was', 'to', 'determine', 'the', 'association', 'between', 'p2x7r', 'rs3751142', 'and', 'card8', 'rs2043211', 'polymorphisms', 'and', 'gout', 'susceptibility', 'in', 'male', 'korean', 'subjects', '.', 'this', 'study', 'enrolled', 'a', 'total', 'of', '242', 'male', 'patients', 'with', 'gout', 'and', '280', 'healthy', 'controls', '.', 'the', 'polymorphisms', 'of', 'two', 'individual', 'genes', 'including', 'rs3751142(c', '>', 'a', ')', 'in', 'the', 'p2x7r', 'gene', 'and', 'rs2043211(a', '>', 't', ')', 'in', 'the', 'card8', 'gene', 'were', 'assessed', 'using', 'taq', '-', 'man', 'analysis', '.', 'statistical', 'analyses', 'were', 'performed', 'using', 'the', 'chi', '-', 'square', 'test', ',', 'kruskal', '-', 'wallis', 'test', ',', 'and', 'logistic', 'regression', 'analyses', '.', 'a', 'difference', 'in', 'genotypic', 'frequency', 'of', 'the', 'p2x7r', 'rs3751142', 'and', 'card8', 'rs2043211', 'genes', 'was', 'not', 'detected', 'between', 'gout', 'and', 'control', 'patients', '.', 'clinical', 'parameters', 'including', 'age', ',', 'onset', 'age', ',', 'disease', 'duration', ',', 'body', 'mass', 'index', ',', 'and', 'serum', 'uric', 'acid', 'levels', 'were', 'not', 'different', 'among', 'the', 'three', 'genotypes', 'for', 'either', 'p2x7r', 'or', 'card8', '(', 'p', '>', '0.05', 'for', 'all', ')', '.', 'a', 'pair', '-', 'wise', 'comparison', 'of', 'p2x7r', 'rs3751142', 'and', 'card8', 'rs2043211', 'genotype', 'combinations', 'revealed', 'that', 'subjects', 'with', 'the', 'ca', 'p2x7r', 'rs3751142', 'genotype', 'and', 'the', 'tt', 'card8', 'rs2043211', 'genotype', 'had', 'a', 'trend', 'toward', 'a', 'higher', 'risk', 'of', 'gout', 'compared', 'to', 'the', 'cc', '/', 'aa', 'combination', '(', 'p', '=', '0.056', ',', 'or', '=', '2.618', ',', '95', '%', 'ci', '0.975', '-', '7.031', ')', '.', 'in', 'conclusion', ',', 'this', 'study', 'revealed', 'that', 'genetic', 'variability', 'of', 'the', 'p2x7r', 'rs3751142', 'and', 'card8', 'rs2043211', 'genes', 'might', ',', 'in', 'part', ',', 'be', 'associated', 'with', 'susceptibility', 'for', 'gout', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0GqkNL5nVAP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "92c772e9-7f4c-41e2-c352-67c6082bbd38"
      },
      "source": [
        "translation, attention = translate_sentence(src, TEXT, TEXT, model, device)\n",
        "\n",
        "print(f'predicted trg = {translation}')\n",
        "reference = ' '.join(trg)\n",
        "prediction = ' '.join(translation)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted trg = [' ', 'the', 'aim', 'of', 'this', 'study', 'was', 'to', 'investigate', 'the', 'association', 'between', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'in', 'the', 'susceptibility', 'to', 'the', 'inflammatory', '<unk>', 'and', '<unk>', '-', 'a', 'polymorphisms', 'of', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'in', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'in', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'in', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'in', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'in', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', '<unk>', 'and', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'in', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'in', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', '<unk>', 'and', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'in', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'in', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'in', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'in', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', '<unk>', 'and', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'in', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'in', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'in', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'in', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'in', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'in', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'in', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'in', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'in', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'in', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'in', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'in', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'in', 'the', 'polymorphisms', 'of', 'the', 'polymorphisms', 'in', 'the', 'susceptibility', 'in', 'the', 'susceptibility', 'to', 'the', 'susceptibility', 'to', 'the', 'susceptibility', 'to', 'the', 'susceptibility', 'gene', '.', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_s9ZCa3Pz_P",
        "colab_type": "text"
      },
      "source": [
        "Compute ROUGE score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyWuSqccgZEq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "186c0bca-a916-4dd1-f54a-723e4faa7731"
      },
      "source": [
        "pip install rouge"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rouge in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rouge) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jOgyIaajBsC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "0aa7528b-293b-4382-dcf0-97c0eb9332e7"
      },
      "source": [
        "from rouge import Rouge\n",
        "import numpy as np\n",
        "\n",
        "rouge1_scores = []\n",
        "rouge2_scores = []\n",
        "rougel_scores = []\n",
        "rouge = Rouge()\n",
        "for example_idx in range(len(test_data.examples)):\n",
        "    src = vars(test_data.examples[example_idx])['INPUT_TEXT']\n",
        "    trg = vars(test_data.examples[example_idx])['LABEL']\n",
        "    translation, attention = translate_sentence(src, TEXT, LABEL, model, device)\n",
        "    reference = ' '.join(trg)\n",
        "    prediction = ' '.join(translation)\n",
        "    try:\n",
        "        scores = rouge.get_scores(prediction, reference)\n",
        "        rouge1_scores.append(scores[0]['rouge-1']['f'])\n",
        "        rouge2_scores.append(scores[0]['rouge-2']['f'])\n",
        "        rougel_scores.append(scores[0]['rouge-l']['f'])\n",
        "    except:\n",
        "        continue\n",
        "print('ROUGE-1 score:{}'.format(np.mean(np.array(rouge1_scores))))\n",
        "print('ROUGE-2 score:{}'.format(np.mean(np.array(rouge2_scores))))\n",
        "print('ROUGE-l score:{}'.format(np.mean(np.array(rougel_scores))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROUGE-1 score:0.4464429571609334\n",
            "ROUGE-2 score:0.3223570913521733\n",
            "ROUGE-l score:0.4142023281695927\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-0ygQxlP7_X",
        "colab_type": "text"
      },
      "source": [
        "Compute Number Recall/Precision."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsbo6EOIO2dU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install word2number"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paHzWo-wOzQK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_split(text):\n",
        "    return text.split(' ')\n",
        "def number_recall(pre,ref,n=4,threshold=0.7):\n",
        "    prediction = pre\n",
        "    reference = ref\n",
        "    pre_num = []\n",
        "    ref_num = []\n",
        "    pre_toks = tokenize_split(prediction)\n",
        "    ref_toks = tokenize_split(reference)\n",
        "    pre_nums_count = 0\n",
        "    ref_nums_count = 0\n",
        "    # extract numbers in prediction\n",
        "    for idx,tok in enumerate(pre_toks):\n",
        "        numbers = re.findall(r'\\d+[\\,\\d+]{1,}|\\d+\\.?\\d+|\\d+',tok)\n",
        "        if(len(numbers)!=0):\n",
        "            pre_nums_count += 1\n",
        "            pre_num.append((numbers[0],idx))\n",
        "        \n",
        "    # extract numbers in reference\n",
        "    for idx,tok in enumerate(ref_toks):\n",
        "        numbers = re.findall(r'\\d+[\\,\\d+]{1,}|\\d+\\.?\\d+|\\d+',tok)\n",
        "        if(len(numbers)!=0):\n",
        "            ref_nums_count += 1\n",
        "            ref_num.append((numbers[0],idx))      \n",
        "#     print('numbers in reference:{}'.format(ref_num))\n",
        "#     print('')\n",
        "#     print('numbers in prediction:{}'.format(pre_num))\n",
        "    rec_num_count = 0\n",
        "    rec_nums = []\n",
        "    for r_num,r_idx in (ref_num):\n",
        "        for p_num,p_idx in pre_num:\n",
        "            if r_num == p_num:\n",
        "                ref_info = []\n",
        "                pre_info = []\n",
        "                ref_info.append(ref_toks[r_idx])\n",
        "                pre_info.append(pre_toks[p_idx])\n",
        "                #提取两边的token\n",
        "                n_gram = 1\n",
        "                while n_gram<n+1 and r_idx-n_gram>=0 and p_idx-n_gram>=0:\n",
        "                    ref_info.insert(0,ref_toks[r_idx-n_gram])\n",
        "                    pre_info.insert(0,pre_toks[p_idx-n_gram])\n",
        "                    n_gram += 1\n",
        "                n_gram = 1\n",
        "                while n_gram<n+1 and r_idx+n_gram<len(ref_toks) and p_idx+n_gram<len(pre_toks):\n",
        "                    ref_info.append(ref_toks[r_idx+n_gram])\n",
        "                    pre_info.append(pre_toks[p_idx+n_gram])\n",
        "                    n_gram += 1\n",
        "                if len(ref_info)==0 or len(pre_info)==0:\n",
        "                    continue\n",
        "                # tokens -> sentence\n",
        "                rs = ' '.join(ref_info)\n",
        "                ps = ' '.join(pre_info)\n",
        "                # compute rouge scores and use f1 measure \n",
        "                rouge = Rouge()\n",
        "                scores = rouge.get_scores(ps, rs)\n",
        "                rg_f = 0\n",
        "                for keys in scores[0]:\n",
        "                    rg_f += scores[0][keys]['f']\n",
        "                if rg_f/3>threshold:\n",
        "                    rec_num_count += 1\n",
        "                    rec_nums.append(r_num)\n",
        "                    \n",
        "                    break\n",
        "    if ref_nums_count==0:\n",
        "        rec_rate = False\n",
        "    else:\n",
        "        rec_rate = rec_num_count/ref_nums_count\n",
        "    if pre_nums_count==0:\n",
        "        pre_rate = False\n",
        "    else:\n",
        "        pre_rate = rec_num_count/pre_nums_count\n",
        "    return rec_rate,pre_rate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJ5iJQ7UO6jo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "1a1befc9-101f-4e80-bf3e-db325de2805c"
      },
      "source": [
        "from rouge import Rouge\n",
        "import numpy as np\n",
        "num_rec_scores = []\n",
        "num_pre_scores = []\n",
        "rouge = Rouge()\n",
        "for example_idx in range(len(valid_data.examples)):\n",
        "    src = vars(valid_data.examples[example_idx])['INPUT_TEXT']\n",
        "    trg = vars(valid_data.examples[example_idx])['LABEL']\n",
        "    translation, attention = translate_sentence(src, TEXT, TEXT, model, device)\n",
        "    reference = ' '.join(trg)\n",
        "    prediction = ' '.join(translation)\n",
        "    # try:\n",
        "    #     scores = number_recall(prediction,reference)\n",
        "    #     if scores is False:\n",
        "    #         continue\n",
        "    #     num_rec_scores.append(scores)\n",
        "    # except:\n",
        "    #     continue\n",
        "    scores,pre= number_recall(prediction,reference)\n",
        "    if scores is False:\n",
        "        a=1\n",
        "    else:\n",
        "        num_rec_scores.append(scores)\n",
        "    if pre is False:\n",
        "        a=1\n",
        "    else:\n",
        "        num_pre_scores.append(pre)\n",
        "print('Number recall score:{}'.format(np.mean(np.array(num_rec_scores))))\n",
        "print('Number recall score std:{}'.format(np.std(np.array(num_rec_scores))))\n",
        "print('Number precision score:{}'.format(np.mean(np.array(num_pre_scores))))\n",
        "print('Number precision score std:{}'.format(np.std(np.array(num_pre_scores))))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number recall score:0.0008609399190753431\n",
            "Number recall score std:0.011884427361945926\n",
            "Number precision score:0.0054432896648321395\n",
            "Number precision score std:0.08038143444250825\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}